{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math Equation Solver2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPk9AV0stgfxZT7tFVjxCcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanGKulkarni/Math-Equation-solver/blob/master/Math_Equation_Solver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading...\")\n",
        "\n",
        "# common libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# CV and Image\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# pickle\n",
        "import pickle\n",
        "\n",
        "# keras\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import *\n",
        "from keras.layers import Input, Dense, Dropout, Flatten\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "K.image_data_format()\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FosRwpNg4_8",
        "outputId": "13216a80-7e5f-4817-de5c-6b2f856d216c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading...\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/train_handwritten.csv',index_col=False)\n",
        "labels = data[['label']]\n",
        "\n",
        "data.drop(data.columns[[784]],axis=1,inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "vOVHcB9zg7aD",
        "outputId": "fe754c6f-f59c-4db0-ccec-ef6e5fbec2eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  28x19  28x20  \\\n",
              "0    0    0    0    0   18  255  255  255  255   255  ...      0      0   \n",
              "1    0    0    0    0    1  255  255  255  255   255  ...      0      0   \n",
              "2    0    0    0    0    0    0  113  132  185   255  ...      0      0   \n",
              "3    0    0    0    0    0    0    0  255  255   255  ...      0      0   \n",
              "4    0    0  198  255  255  255  255  111    0     0  ...      0      0   \n",
              "\n",
              "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
              "0      0      0      0      0      0      0      0      0  \n",
              "1      0      0      0      0      0      0      0      0  \n",
              "2      0      0      0      0      0      0      0      0  \n",
              "3      0      0      0      0      0      0      0      0  \n",
              "4      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bbd8757-f24a-4bba-82a5-adc97a0bd716\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>1x10</th>\n",
              "      <th>...</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>113</td>\n",
              "      <td>132</td>\n",
              "      <td>185</td>\n",
              "      <td>255</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>198</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>111</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 784 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bbd8757-f24a-4bba-82a5-adc97a0bd716')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1bbd8757-f24a-4bba-82a5-adc97a0bd716 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1bbd8757-f24a-4bba-82a5-adc97a0bd716');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1212)\n",
        "labels=np.array(labels)\n",
        "cat=to_categorical(labels,num_classes=19)\n",
        "cat[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QfTfKephAtg",
        "outputId": "3d466d3f-46a7-45bb-d112-81781bbc9c8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp=data.to_numpy()\n",
        "X_train = temp.reshape(temp.shape[0], 28, 28, 1)\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeQV0FuNhIYx",
        "outputId": "f227d906-9562-4a14-92c8-63470786d6f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10071, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=[]\n",
        "for i in range(X_train.shape[0]):\n",
        "    l.append(np.array(data[i:i+1]).reshape(1,28,28))\n",
        "\n",
        "np.random.seed(7)"
      ],
      "metadata": {
        "id": "m2ILvKMehMHt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), input_shape=(28, 28, 1), activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(19, activation='softmax'))\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"eq_solver.h5\", monitor='accuracy', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "nn = model.fit(X_train, cat, epochs=200, batch_size=256, shuffle=True, verbose=1, callbacks=[checkpoint]).history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLOOXA-dixJG",
        "outputId": "4fbf56bf-490b-4484-c5c8-acfda7951d21"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 6.7738 - accuracy: 0.1570\n",
            "Epoch 1: accuracy improved from -inf to 0.15699, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 6.7738 - accuracy: 0.1570\n",
            "Epoch 2/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.9834 - accuracy: 0.3936\n",
            "Epoch 2: accuracy improved from 0.15699 to 0.39361, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 1.9834 - accuracy: 0.3936\n",
            "Epoch 3/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2144 - accuracy: 0.6091\n",
            "Epoch 3: accuracy improved from 0.39361 to 0.60908, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 1.2144 - accuracy: 0.6091\n",
            "Epoch 4/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.8246 - accuracy: 0.7263\n",
            "Epoch 4: accuracy improved from 0.60908 to 0.72634, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.8246 - accuracy: 0.7263\n",
            "Epoch 5/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.7920\n",
            "Epoch 5: accuracy improved from 0.72634 to 0.79198, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.6186 - accuracy: 0.7920\n",
            "Epoch 6/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.8305\n",
            "Epoch 6: accuracy improved from 0.79198 to 0.83050, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.4905 - accuracy: 0.8305\n",
            "Epoch 7/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8505\n",
            "Epoch 7: accuracy improved from 0.83050 to 0.85046, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.4247 - accuracy: 0.8505\n",
            "Epoch 8/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.8707\n",
            "Epoch 8: accuracy improved from 0.85046 to 0.87072, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.3721 - accuracy: 0.8707\n",
            "Epoch 9/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.8787\n",
            "Epoch 9: accuracy improved from 0.87072 to 0.87866, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.3309 - accuracy: 0.8787\n",
            "Epoch 10/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.3050 - accuracy: 0.8885\n",
            "Epoch 10: accuracy improved from 0.87866 to 0.88849, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.3050 - accuracy: 0.8885\n",
            "Epoch 11/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8969\n",
            "Epoch 11: accuracy improved from 0.88849 to 0.89693, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 184ms/step - loss: 0.2779 - accuracy: 0.8969\n",
            "Epoch 12/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9052\n",
            "Epoch 12: accuracy improved from 0.89693 to 0.90517, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 171ms/step - loss: 0.2506 - accuracy: 0.9052\n",
            "Epoch 13/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9075\n",
            "Epoch 13: accuracy improved from 0.90517 to 0.90746, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.2382 - accuracy: 0.9075\n",
            "Epoch 14/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9106\n",
            "Epoch 14: accuracy improved from 0.90746 to 0.91063, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.2303 - accuracy: 0.9106\n",
            "Epoch 15/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.9167\n",
            "Epoch 15: accuracy improved from 0.91063 to 0.91669, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.2105 - accuracy: 0.9167\n",
            "Epoch 16/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9224\n",
            "Epoch 16: accuracy improved from 0.91669 to 0.92235, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.2053 - accuracy: 0.9224\n",
            "Epoch 17/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1971 - accuracy: 0.9244\n",
            "Epoch 17: accuracy improved from 0.92235 to 0.92444, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1971 - accuracy: 0.9244\n",
            "Epoch 18/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9279\n",
            "Epoch 18: accuracy improved from 0.92444 to 0.92791, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 169ms/step - loss: 0.1872 - accuracy: 0.9279\n",
            "Epoch 19/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9291\n",
            "Epoch 19: accuracy improved from 0.92791 to 0.92910, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 8s 204ms/step - loss: 0.1847 - accuracy: 0.9291\n",
            "Epoch 20/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9335\n",
            "Epoch 20: accuracy improved from 0.92910 to 0.93347, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.1725 - accuracy: 0.9335\n",
            "Epoch 21/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9324\n",
            "Epoch 21: accuracy did not improve from 0.93347\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.1672 - accuracy: 0.9324\n",
            "Epoch 22/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9316\n",
            "Epoch 22: accuracy did not improve from 0.93347\n",
            "40/40 [==============================] - 6s 161ms/step - loss: 0.1718 - accuracy: 0.9316\n",
            "Epoch 23/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9383\n",
            "Epoch 23: accuracy improved from 0.93347 to 0.93834, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1565 - accuracy: 0.9383\n",
            "Epoch 24/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9397\n",
            "Epoch 24: accuracy improved from 0.93834 to 0.93973, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1481 - accuracy: 0.9397\n",
            "Epoch 25/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9373\n",
            "Epoch 25: accuracy did not improve from 0.93973\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.1531 - accuracy: 0.9373\n",
            "Epoch 26/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9394\n",
            "Epoch 26: accuracy did not improve from 0.93973\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.1499 - accuracy: 0.9394\n",
            "Epoch 27/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9426\n",
            "Epoch 27: accuracy improved from 0.93973 to 0.94261, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1400 - accuracy: 0.9426\n",
            "Epoch 28/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9422\n",
            "Epoch 28: accuracy did not improve from 0.94261\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.1406 - accuracy: 0.9422\n",
            "Epoch 29/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9439\n",
            "Epoch 29: accuracy improved from 0.94261 to 0.94390, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.1380 - accuracy: 0.9439\n",
            "Epoch 30/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9442\n",
            "Epoch 30: accuracy improved from 0.94390 to 0.94420, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1369 - accuracy: 0.9442\n",
            "Epoch 31/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9477\n",
            "Epoch 31: accuracy improved from 0.94420 to 0.94767, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.1294 - accuracy: 0.9477\n",
            "Epoch 32/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9480\n",
            "Epoch 32: accuracy improved from 0.94767 to 0.94797, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.1325 - accuracy: 0.9480\n",
            "Epoch 33/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9488\n",
            "Epoch 33: accuracy improved from 0.94797 to 0.94876, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.1252 - accuracy: 0.9488\n",
            "Epoch 34/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9511\n",
            "Epoch 34: accuracy improved from 0.94876 to 0.95115, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1186 - accuracy: 0.9511\n",
            "Epoch 35/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9509\n",
            "Epoch 35: accuracy did not improve from 0.95115\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.1156 - accuracy: 0.9509\n",
            "Epoch 36/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9501\n",
            "Epoch 36: accuracy did not improve from 0.95115\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.1202 - accuracy: 0.9501\n",
            "Epoch 37/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9522\n",
            "Epoch 37: accuracy improved from 0.95115 to 0.95224, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 9s 215ms/step - loss: 0.1164 - accuracy: 0.9522\n",
            "Epoch 38/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9529\n",
            "Epoch 38: accuracy improved from 0.95224 to 0.95293, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.1152 - accuracy: 0.9529\n",
            "Epoch 39/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9545\n",
            "Epoch 39: accuracy improved from 0.95293 to 0.95452, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.1155 - accuracy: 0.9545\n",
            "Epoch 40/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9545\n",
            "Epoch 40: accuracy did not improve from 0.95452\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.1116 - accuracy: 0.9545\n",
            "Epoch 41/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9573\n",
            "Epoch 41: accuracy improved from 0.95452 to 0.95730, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.1081 - accuracy: 0.9573\n",
            "Epoch 42/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9558\n",
            "Epoch 42: accuracy did not improve from 0.95730\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1089 - accuracy: 0.9558\n",
            "Epoch 43/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9552\n",
            "Epoch 43: accuracy did not improve from 0.95730\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.1089 - accuracy: 0.9552\n",
            "Epoch 44/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9595\n",
            "Epoch 44: accuracy improved from 0.95730 to 0.95949, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1063 - accuracy: 0.9595\n",
            "Epoch 45/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9584\n",
            "Epoch 45: accuracy did not improve from 0.95949\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.1040 - accuracy: 0.9584\n",
            "Epoch 46/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9581\n",
            "Epoch 46: accuracy did not improve from 0.95949\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.1032 - accuracy: 0.9581\n",
            "Epoch 47/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9599\n",
            "Epoch 47: accuracy improved from 0.95949 to 0.95988, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0976 - accuracy: 0.9599\n",
            "Epoch 48/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9533\n",
            "Epoch 48: accuracy did not improve from 0.95988\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1083 - accuracy: 0.9533\n",
            "Epoch 49/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9579\n",
            "Epoch 49: accuracy did not improve from 0.95988\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1036 - accuracy: 0.9579\n",
            "Epoch 50/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9573\n",
            "Epoch 50: accuracy did not improve from 0.95988\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.1019 - accuracy: 0.9573\n",
            "Epoch 51/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9636\n",
            "Epoch 51: accuracy improved from 0.95988 to 0.96356, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0927 - accuracy: 0.9636\n",
            "Epoch 52/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9631\n",
            "Epoch 52: accuracy did not improve from 0.96356\n",
            "40/40 [==============================] - 6s 161ms/step - loss: 0.0904 - accuracy: 0.9631\n",
            "Epoch 53/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9627\n",
            "Epoch 53: accuracy did not improve from 0.96356\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0943 - accuracy: 0.9627\n",
            "Epoch 54/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9616\n",
            "Epoch 54: accuracy did not improve from 0.96356\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0880 - accuracy: 0.9616\n",
            "Epoch 55/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9626\n",
            "Epoch 55: accuracy did not improve from 0.96356\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0917 - accuracy: 0.9626\n",
            "Epoch 56/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9634\n",
            "Epoch 56: accuracy did not improve from 0.96356\n",
            "40/40 [==============================] - 8s 213ms/step - loss: 0.0889 - accuracy: 0.9634\n",
            "Epoch 57/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9662\n",
            "Epoch 57: accuracy improved from 0.96356 to 0.96624, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0842 - accuracy: 0.9662\n",
            "Epoch 58/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9663\n",
            "Epoch 58: accuracy improved from 0.96624 to 0.96634, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0838 - accuracy: 0.9663\n",
            "Epoch 59/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9659\n",
            "Epoch 59: accuracy did not improve from 0.96634\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0837 - accuracy: 0.9659\n",
            "Epoch 60/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9695\n",
            "Epoch 60: accuracy improved from 0.96634 to 0.96952, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0812 - accuracy: 0.9695\n",
            "Epoch 61/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9653\n",
            "Epoch 61: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0852 - accuracy: 0.9653\n",
            "Epoch 62/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9661\n",
            "Epoch 62: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0826 - accuracy: 0.9661\n",
            "Epoch 63/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9676\n",
            "Epoch 63: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0815 - accuracy: 0.9676\n",
            "Epoch 64/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9670\n",
            "Epoch 64: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0811 - accuracy: 0.9670\n",
            "Epoch 65/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9663\n",
            "Epoch 65: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0832 - accuracy: 0.9663\n",
            "Epoch 66/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9676\n",
            "Epoch 66: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0841 - accuracy: 0.9676\n",
            "Epoch 67/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9690\n",
            "Epoch 67: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0799 - accuracy: 0.9690\n",
            "Epoch 68/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9682\n",
            "Epoch 68: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0791 - accuracy: 0.9682\n",
            "Epoch 69/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9668\n",
            "Epoch 69: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0852 - accuracy: 0.9668\n",
            "Epoch 70/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9650\n",
            "Epoch 70: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0825 - accuracy: 0.9650\n",
            "Epoch 71/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9688\n",
            "Epoch 71: accuracy did not improve from 0.96952\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0784 - accuracy: 0.9688\n",
            "Epoch 72/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9726\n",
            "Epoch 72: accuracy improved from 0.96952 to 0.97259, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0713 - accuracy: 0.9726\n",
            "Epoch 73/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9728\n",
            "Epoch 73: accuracy improved from 0.97259 to 0.97279, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0672 - accuracy: 0.9728\n",
            "Epoch 74/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9698\n",
            "Epoch 74: accuracy did not improve from 0.97279\n",
            "40/40 [==============================] - 9s 214ms/step - loss: 0.0763 - accuracy: 0.9698\n",
            "Epoch 75/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9690\n",
            "Epoch 75: accuracy did not improve from 0.97279\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0773 - accuracy: 0.9690\n",
            "Epoch 76/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9664\n",
            "Epoch 76: accuracy did not improve from 0.97279\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0788 - accuracy: 0.9664\n",
            "Epoch 77/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9717\n",
            "Epoch 77: accuracy did not improve from 0.97279\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0762 - accuracy: 0.9717\n",
            "Epoch 78/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9729\n",
            "Epoch 78: accuracy improved from 0.97279 to 0.97289, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0710 - accuracy: 0.9729\n",
            "Epoch 79/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9705\n",
            "Epoch 79: accuracy did not improve from 0.97289\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0708 - accuracy: 0.9705\n",
            "Epoch 80/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9747\n",
            "Epoch 80: accuracy improved from 0.97289 to 0.97468, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0626 - accuracy: 0.9747\n",
            "Epoch 81/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9742\n",
            "Epoch 81: accuracy did not improve from 0.97468\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0640 - accuracy: 0.9742\n",
            "Epoch 82/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9740\n",
            "Epoch 82: accuracy did not improve from 0.97468\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0650 - accuracy: 0.9740\n",
            "Epoch 83/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9736\n",
            "Epoch 83: accuracy did not improve from 0.97468\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0663 - accuracy: 0.9736\n",
            "Epoch 84/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9733\n",
            "Epoch 84: accuracy did not improve from 0.97468\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0666 - accuracy: 0.9733\n",
            "Epoch 85/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9763\n",
            "Epoch 85: accuracy improved from 0.97468 to 0.97627, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0609 - accuracy: 0.9763\n",
            "Epoch 86/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9761\n",
            "Epoch 86: accuracy did not improve from 0.97627\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0593 - accuracy: 0.9761\n",
            "Epoch 87/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9743\n",
            "Epoch 87: accuracy did not improve from 0.97627\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.0669 - accuracy: 0.9743\n",
            "Epoch 88/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9728\n",
            "Epoch 88: accuracy did not improve from 0.97627\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0702 - accuracy: 0.9728\n",
            "Epoch 89/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9743\n",
            "Epoch 89: accuracy did not improve from 0.97627\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0683 - accuracy: 0.9743\n",
            "Epoch 90/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9764\n",
            "Epoch 90: accuracy improved from 0.97627 to 0.97637, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0592 - accuracy: 0.9764\n",
            "Epoch 91/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9741\n",
            "Epoch 91: accuracy did not improve from 0.97637\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0657 - accuracy: 0.9741\n",
            "Epoch 92/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9752\n",
            "Epoch 92: accuracy did not improve from 0.97637\n",
            "40/40 [==============================] - 8s 190ms/step - loss: 0.0652 - accuracy: 0.9752\n",
            "Epoch 93/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9766\n",
            "Epoch 93: accuracy improved from 0.97637 to 0.97657, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 8s 186ms/step - loss: 0.0591 - accuracy: 0.9766\n",
            "Epoch 94/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9771\n",
            "Epoch 94: accuracy improved from 0.97657 to 0.97706, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0581 - accuracy: 0.9771\n",
            "Epoch 95/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9754\n",
            "Epoch 95: accuracy did not improve from 0.97706\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0634 - accuracy: 0.9754\n",
            "Epoch 96/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9761\n",
            "Epoch 96: accuracy did not improve from 0.97706\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0622 - accuracy: 0.9761\n",
            "Epoch 97/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9789\n",
            "Epoch 97: accuracy improved from 0.97706 to 0.97895, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0571 - accuracy: 0.9789\n",
            "Epoch 98/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9762\n",
            "Epoch 98: accuracy did not improve from 0.97895\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.0617 - accuracy: 0.9762\n",
            "Epoch 99/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9782\n",
            "Epoch 99: accuracy did not improve from 0.97895\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0559 - accuracy: 0.9782\n",
            "Epoch 100/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9768\n",
            "Epoch 100: accuracy did not improve from 0.97895\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0583 - accuracy: 0.9768\n",
            "Epoch 101/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9783\n",
            "Epoch 101: accuracy did not improve from 0.97895\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0554 - accuracy: 0.9783\n",
            "Epoch 102/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9789\n",
            "Epoch 102: accuracy did not improve from 0.97895\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0541 - accuracy: 0.9789\n",
            "Epoch 103/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9772\n",
            "Epoch 103: accuracy did not improve from 0.97895\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0598 - accuracy: 0.9772\n",
            "Epoch 104/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9793\n",
            "Epoch 104: accuracy improved from 0.97895 to 0.97935, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0539 - accuracy: 0.9793\n",
            "Epoch 105/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9831\n",
            "Epoch 105: accuracy improved from 0.97935 to 0.98312, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0456 - accuracy: 0.9831\n",
            "Epoch 106/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9807\n",
            "Epoch 106: accuracy did not improve from 0.98312\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0483 - accuracy: 0.9807\n",
            "Epoch 107/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9799\n",
            "Epoch 107: accuracy did not improve from 0.98312\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0529 - accuracy: 0.9799\n",
            "Epoch 108/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9859\n",
            "Epoch 108: accuracy improved from 0.98312 to 0.98590, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0424 - accuracy: 0.9859\n",
            "Epoch 109/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9808\n",
            "Epoch 109: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 6s 163ms/step - loss: 0.0514 - accuracy: 0.9808\n",
            "Epoch 110/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9793\n",
            "Epoch 110: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0512 - accuracy: 0.9793\n",
            "Epoch 111/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9826\n",
            "Epoch 111: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 9s 215ms/step - loss: 0.0492 - accuracy: 0.9826\n",
            "Epoch 112/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9800\n",
            "Epoch 112: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0561 - accuracy: 0.9800\n",
            "Epoch 113/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9827\n",
            "Epoch 113: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0447 - accuracy: 0.9827\n",
            "Epoch 114/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9807\n",
            "Epoch 114: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0517 - accuracy: 0.9807\n",
            "Epoch 115/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9821\n",
            "Epoch 115: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0493 - accuracy: 0.9821\n",
            "Epoch 116/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9814\n",
            "Epoch 116: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0513 - accuracy: 0.9814\n",
            "Epoch 117/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9818\n",
            "Epoch 117: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0493 - accuracy: 0.9818\n",
            "Epoch 118/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9819\n",
            "Epoch 118: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 162ms/step - loss: 0.0460 - accuracy: 0.9819\n",
            "Epoch 119/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9825\n",
            "Epoch 119: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0440 - accuracy: 0.9825\n",
            "Epoch 120/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9833\n",
            "Epoch 120: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0431 - accuracy: 0.9833\n",
            "Epoch 121/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9845\n",
            "Epoch 121: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0406 - accuracy: 0.9845\n",
            "Epoch 122/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9843\n",
            "Epoch 122: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0407 - accuracy: 0.9843\n",
            "Epoch 123/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9841\n",
            "Epoch 123: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0437 - accuracy: 0.9841\n",
            "Epoch 124/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9849\n",
            "Epoch 124: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0422 - accuracy: 0.9849\n",
            "Epoch 125/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9829\n",
            "Epoch 125: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 6s 162ms/step - loss: 0.0460 - accuracy: 0.9829\n",
            "Epoch 126/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9816\n",
            "Epoch 126: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0476 - accuracy: 0.9816\n",
            "Epoch 127/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9847\n",
            "Epoch 127: accuracy did not improve from 0.98590\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0430 - accuracy: 0.9847\n",
            "Epoch 128/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9860\n",
            "Epoch 128: accuracy improved from 0.98590 to 0.98600, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0391 - accuracy: 0.9860\n",
            "Epoch 129/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9838\n",
            "Epoch 129: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 8s 200ms/step - loss: 0.0462 - accuracy: 0.9838\n",
            "Epoch 130/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9834\n",
            "Epoch 130: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 7s 175ms/step - loss: 0.0461 - accuracy: 0.9834\n",
            "Epoch 131/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9842\n",
            "Epoch 131: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0449 - accuracy: 0.9842\n",
            "Epoch 132/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9844\n",
            "Epoch 132: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0421 - accuracy: 0.9844\n",
            "Epoch 133/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9821\n",
            "Epoch 133: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0474 - accuracy: 0.9821\n",
            "Epoch 134/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9850\n",
            "Epoch 134: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0436 - accuracy: 0.9850\n",
            "Epoch 135/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9851\n",
            "Epoch 135: accuracy did not improve from 0.98600\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0380 - accuracy: 0.9851\n",
            "Epoch 136/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9868\n",
            "Epoch 136: accuracy improved from 0.98600 to 0.98679, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0385 - accuracy: 0.9868\n",
            "Epoch 137/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9844\n",
            "Epoch 137: accuracy did not improve from 0.98679\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0418 - accuracy: 0.9844\n",
            "Epoch 138/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9837\n",
            "Epoch 138: accuracy did not improve from 0.98679\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0480 - accuracy: 0.9837\n",
            "Epoch 139/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9827\n",
            "Epoch 139: accuracy did not improve from 0.98679\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0468 - accuracy: 0.9827\n",
            "Epoch 140/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9868\n",
            "Epoch 140: accuracy did not improve from 0.98679\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0375 - accuracy: 0.9868\n",
            "Epoch 141/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9861\n",
            "Epoch 141: accuracy did not improve from 0.98679\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0402 - accuracy: 0.9861\n",
            "Epoch 142/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9867\n",
            "Epoch 142: accuracy did not improve from 0.98679\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0430 - accuracy: 0.9867\n",
            "Epoch 143/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9893\n",
            "Epoch 143: accuracy improved from 0.98679 to 0.98928, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0296 - accuracy: 0.9893\n",
            "Epoch 144/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9875\n",
            "Epoch 144: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0382 - accuracy: 0.9875\n",
            "Epoch 145/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9860\n",
            "Epoch 145: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0402 - accuracy: 0.9860\n",
            "Epoch 146/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9885\n",
            "Epoch 146: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 163ms/step - loss: 0.0325 - accuracy: 0.9885\n",
            "Epoch 147/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9841\n",
            "Epoch 147: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0383 - accuracy: 0.9841\n",
            "Epoch 148/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9864\n",
            "Epoch 148: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 9s 214ms/step - loss: 0.0378 - accuracy: 0.9864\n",
            "Epoch 149/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9844\n",
            "Epoch 149: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0465 - accuracy: 0.9844\n",
            "Epoch 150/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9873\n",
            "Epoch 150: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0353 - accuracy: 0.9873\n",
            "Epoch 151/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9874\n",
            "Epoch 151: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0371 - accuracy: 0.9874\n",
            "Epoch 152/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9862\n",
            "Epoch 152: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0353 - accuracy: 0.9862\n",
            "Epoch 153/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9876\n",
            "Epoch 153: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0383 - accuracy: 0.9876\n",
            "Epoch 154/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9888\n",
            "Epoch 154: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0350 - accuracy: 0.9888\n",
            "Epoch 155/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9876\n",
            "Epoch 155: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0344 - accuracy: 0.9876\n",
            "Epoch 156/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9859\n",
            "Epoch 156: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0373 - accuracy: 0.9859\n",
            "Epoch 157/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9891\n",
            "Epoch 157: accuracy did not improve from 0.98928\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0305 - accuracy: 0.9891\n",
            "Epoch 158/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9902\n",
            "Epoch 158: accuracy improved from 0.98928 to 0.99017, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0287 - accuracy: 0.9902\n",
            "Epoch 159/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9885\n",
            "Epoch 159: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0335 - accuracy: 0.9885\n",
            "Epoch 160/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9881\n",
            "Epoch 160: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0368 - accuracy: 0.9881\n",
            "Epoch 161/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9858\n",
            "Epoch 161: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0346 - accuracy: 0.9858\n",
            "Epoch 162/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9898\n",
            "Epoch 162: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0295 - accuracy: 0.9898\n",
            "Epoch 163/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9884\n",
            "Epoch 163: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0326 - accuracy: 0.9884\n",
            "Epoch 164/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9843\n",
            "Epoch 164: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0465 - accuracy: 0.9843\n",
            "Epoch 165/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9890\n",
            "Epoch 165: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0302 - accuracy: 0.9890\n",
            "Epoch 166/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9879\n",
            "Epoch 166: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 9s 215ms/step - loss: 0.0358 - accuracy: 0.9879\n",
            "Epoch 167/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9885\n",
            "Epoch 167: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0362 - accuracy: 0.9885\n",
            "Epoch 168/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9868\n",
            "Epoch 168: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0381 - accuracy: 0.9868\n",
            "Epoch 169/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9878\n",
            "Epoch 169: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0354 - accuracy: 0.9878\n",
            "Epoch 170/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9900\n",
            "Epoch 170: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 164ms/step - loss: 0.0310 - accuracy: 0.9900\n",
            "Epoch 171/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9878\n",
            "Epoch 171: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0383 - accuracy: 0.9878\n",
            "Epoch 172/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9895\n",
            "Epoch 172: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0292 - accuracy: 0.9895\n",
            "Epoch 173/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9884\n",
            "Epoch 173: accuracy did not improve from 0.99017\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0300 - accuracy: 0.9884\n",
            "Epoch 174/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9916\n",
            "Epoch 174: accuracy improved from 0.99017 to 0.99156, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0209 - accuracy: 0.9916\n",
            "Epoch 175/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9893\n",
            "Epoch 175: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0310 - accuracy: 0.9893\n",
            "Epoch 176/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9893\n",
            "Epoch 176: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0301 - accuracy: 0.9893\n",
            "Epoch 177/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9882\n",
            "Epoch 177: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0340 - accuracy: 0.9882\n",
            "Epoch 178/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9910\n",
            "Epoch 178: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0273 - accuracy: 0.9910\n",
            "Epoch 179/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9886\n",
            "Epoch 179: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0295 - accuracy: 0.9886\n",
            "Epoch 180/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9878\n",
            "Epoch 180: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0323 - accuracy: 0.9878\n",
            "Epoch 181/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9901\n",
            "Epoch 181: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0305 - accuracy: 0.9901\n",
            "Epoch 182/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9893\n",
            "Epoch 182: accuracy did not improve from 0.99156\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0311 - accuracy: 0.9893\n",
            "Epoch 183/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9919\n",
            "Epoch 183: accuracy improved from 0.99156 to 0.99186, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 175ms/step - loss: 0.0259 - accuracy: 0.9919\n",
            "Epoch 184/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9907\n",
            "Epoch 184: accuracy did not improve from 0.99186\n",
            "40/40 [==============================] - 9s 211ms/step - loss: 0.0251 - accuracy: 0.9907\n",
            "Epoch 185/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9895\n",
            "Epoch 185: accuracy did not improve from 0.99186\n",
            "40/40 [==============================] - 7s 169ms/step - loss: 0.0299 - accuracy: 0.9895\n",
            "Epoch 186/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9881\n",
            "Epoch 186: accuracy did not improve from 0.99186\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0377 - accuracy: 0.9881\n",
            "Epoch 187/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9890\n",
            "Epoch 187: accuracy did not improve from 0.99186\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0353 - accuracy: 0.9890\n",
            "Epoch 188/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9932\n",
            "Epoch 188: accuracy improved from 0.99186 to 0.99325, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 168ms/step - loss: 0.0217 - accuracy: 0.9932\n",
            "Epoch 189/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9906\n",
            "Epoch 189: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 165ms/step - loss: 0.0306 - accuracy: 0.9906\n",
            "Epoch 190/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9894\n",
            "Epoch 190: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0335 - accuracy: 0.9894\n",
            "Epoch 191/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9923\n",
            "Epoch 191: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0228 - accuracy: 0.9923\n",
            "Epoch 192/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9916\n",
            "Epoch 192: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0258 - accuracy: 0.9916\n",
            "Epoch 193/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9904\n",
            "Epoch 193: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0281 - accuracy: 0.9904\n",
            "Epoch 194/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9916\n",
            "Epoch 194: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0265 - accuracy: 0.9916\n",
            "Epoch 195/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9915\n",
            "Epoch 195: accuracy did not improve from 0.99325\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0260 - accuracy: 0.9915\n",
            "Epoch 196/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9935\n",
            "Epoch 196: accuracy improved from 0.99325 to 0.99355, saving model to eq_solver.h5\n",
            "40/40 [==============================] - 7s 168ms/step - loss: 0.0194 - accuracy: 0.9935\n",
            "Epoch 197/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9919\n",
            "Epoch 197: accuracy did not improve from 0.99355\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0242 - accuracy: 0.9919\n",
            "Epoch 198/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9904\n",
            "Epoch 198: accuracy did not improve from 0.99355\n",
            "40/40 [==============================] - 7s 167ms/step - loss: 0.0257 - accuracy: 0.9904\n",
            "Epoch 199/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9907\n",
            "Epoch 199: accuracy did not improve from 0.99355\n",
            "40/40 [==============================] - 7s 168ms/step - loss: 0.0251 - accuracy: 0.9907\n",
            "Epoch 200/200\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9928\n",
            "Epoch 200: accuracy did not improve from 0.99355\n",
            "40/40 [==============================] - 7s 166ms/step - loss: 0.0197 - accuracy: 0.9928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        " \n",
        " \n",
        "# Save the model as a pickle in a file\n",
        "joblib.dump(model, 'Math equation solver.pkl')\n",
        " \n",
        "# Load the model from the file\n",
        "model = joblib.load('Math equation solver.pkl')\n"
      ],
      "metadata": {
        "id": "9CQxnFlekENh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "from sklearn.tree import BaseDecisionTree\n",
        "\n",
        "# Load the model from the file\n",
        "model = joblib.load('random_forest_classifier.pkl')"
      ],
      "metadata": {
        "id": "cRt7heiJkFoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/Untitled.png',cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "uVNpfg3v6SwB",
        "outputId": "c14b01e6-1f27-45e6-e4ff-3e14bcd96ad8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2be3770650>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if img is not None:\n",
        "    img=~img\n",
        "    _,thresh=cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
        "    ctrs,_=cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
        "    cnt=sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
        "    w=int(28)\n",
        "    h=int(28)\n",
        "    train_data=[]\n",
        "    print(len(cnt))\n",
        "    rects=[]\n",
        "    for c in cnt :\n",
        "        x,y,w,h= cv2.boundingRect(c)\n",
        "        rect=[x,y,w,h]\n",
        "        rects.append(rect)\n",
        "    bool_rect=[]\n",
        "    for r in rects:\n",
        "        l=[]\n",
        "        for rec in rects:\n",
        "            flag=0\n",
        "            if rec!=r:\n",
        "                if r[0]<(rec[0]+rec[2]+10) and rec[0]<(r[0]+r[2]+10) and r[1]<(rec[1]+rec[3]+10) and rec[1]<(r[1]+r[3]+10):\n",
        "                    flag=1\n",
        "                l.append(flag)\n",
        "            if rec==r:\n",
        "                l.append(0)\n",
        "        bool_rect.append(l)\n",
        "    dump_rect=[]\n",
        "    for i in range(0,len(cnt)):\n",
        "        for j in range(0,len(cnt)):\n",
        "            if bool_rect[i][j]==1:\n",
        "                area1=rects[i][2]*rects[i][3]\n",
        "                area2=rects[j][2]*rects[j][3]\n",
        "                if(area1==min(area1,area2)):\n",
        "                    dump_rect.append(rects[i])\n",
        "    print(len(dump_rect)) \n",
        "    final_rect=[i for i in rects if i not in dump_rect]\n",
        "    print(final_rect)\n",
        "    for r in final_rect:\n",
        "        x=r[0]\n",
        "        y=r[1]\n",
        "        w=r[2]\n",
        "        h=r[3]\n",
        "        im_crop =thresh[y:y+h+10,x:x+w+10]\n",
        "        im_resize = cv2.resize(im_crop,(28,28))\n",
        "        im_resize=np.reshape(im_resize,(28,28,1))\n",
        "        train_data.append(im_resize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilA5rEMUkff9",
        "outputId": "f6a9b35e-bff8-4891-b46c-43ec2cbdf700"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "0\n",
            "[[84, 101, 186, 308], [305, 251, 91, 129], [514, 193, 184, 165], [742, 111, 163, 265], [930, 218, 95, 11], [932, 268, 85, 19], [1064, 102, 98, 288]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for digit in train_data:\n",
        "    prediction = model.predict(digit.reshape(1, 28, 28, 1))  \n",
        "    \n",
        "    print (\"\\n\\n---------------------------------------\\n\\n\")\n",
        "    print (\"=========PREDICTION============ \\n\\n\")\n",
        "    plt.imshow(digit.reshape(28, 28), cmap=\"gray\")\n",
        "    plt.show()\n",
        "    print(\"\\n\\nFinal Output: {}\".format(np.argmax(prediction)))\n",
        "    \n",
        "    print (\"\\nPrediction (Softmax) from the neural network:\\n\\n {}\".format(prediction))\n",
        "    \n",
        "    hard_maxed_prediction = np.zeros(prediction.shape)\n",
        "    hard_maxed_prediction[0][np.argmax(prediction)] = 1\n",
        "    print (\"\\n\\nHard-maxed form of the prediction: \\n\\n {}\".format(hard_maxed_prediction))\n",
        "    print (\"\\n\\n---------------------------------------\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "baOm-OLpknb9",
        "outputId": "e7641d7c-aaf1-4622-9201-86a7f08e16a7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 6\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[1.4477033e-24 2.4946623e-30 2.0865190e-20 1.5008309e-18 2.6780451e-31\n",
            "  4.2400621e-18 9.9999928e-01 3.0595407e-33 7.3868551e-07 5.6152583e-27\n",
            "  0.0000000e+00 0.0000000e+00 8.8651030e-26 1.7351505e-21 4.1630206e-25\n",
            "  0.0000000e+00 1.9824680e-19 4.2158114e-30 2.0722700e-30]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 16\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[0.0000000e+00 7.3200740e-17 3.8604040e-13 0.0000000e+00 1.0727872e-24\n",
            "  1.2811169e-22 1.1696601e-23 3.7308273e-21 5.8122460e-20 1.0602526e-31\n",
            "  7.1397252e-28 2.6428073e-30 6.5018475e-02 5.7007155e-21 5.5560647e-25\n",
            "  3.5142451e-33 9.3498158e-01 8.3456441e-31 1.2658119e-19]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 10\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[1.0538569e-36 2.9408384e-20 1.7489727e-30 6.0327980e-37 1.4993384e-18\n",
            "  1.5747358e-35 5.2876999e-28 8.2800867e-15 9.3453873e-29 1.2477700e-25\n",
            "  1.0000000e+00 3.6154422e-36 7.7299877e-33 2.4180286e-17 7.8613576e-28\n",
            "  3.1982753e-25 9.0574932e-26 1.3486455e-19 1.1116192e-19]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMG0lEQVR4nO3dX4hc9RnG8eepjTcm2qTSEGKsWhQRL7QJuclaLNKg3kRvxFyUSEvXi/qnUFAxgoEa0NIqAUthrcFYrCKoNZRATEWaBEHchDR/TFKtREyI2Upau/HGRt9ezEnYxJ2ZzZwzc87u+/3AMDPnNzPn5WSfnD+/c87PESEAM9836i4AwGAQdiAJwg4kQdiBJAg7kMQ3Bzkz2ykP/S9evLjuEmpz4MCBtm2ff/75ACvJIyI82XSX6XqzfbOkdZLOk/SHiHi8y+dThj1z9+ayZcvatr399tsDrCSPdmHveTPe9nmSfifpFknXSFpp+5pefw9Af5XZZ18q6YOI+DAivpD0kqQV1ZQFoGplwr5Q0scT3h8upp3B9rDtUdujJeYFoKS+H6CLiBFJI1LefXagCcqs2Y9IWjTh/SXFNAANVCbs70q60vblts+XdKekjdWUBaBqPW/GR8RJ2/dI2qxW19v6iNhXWWUVe+CBBzq2P/HEEz3/9vbt2zu225P2hKSwbdu2tm1DQ0Olfjvzcu1FqX32iNgkaVNFtQDoI06XBZIg7EAShB1IgrADSRB2IAnCDiRR6hLXc54Zp8viHGzdurVj+w033NCxPWs/fOWXuAKYXgg7kARhB5Ig7EAShB1IgrADSdD1hhmr09/2008/3fG79957b9XlDAxdb0ByhB1IgrADSRB2IAnCDiRB2IEkCDuQBP3smLFmz57dtm18fLzjd6fz5bH0swPJEXYgCcIOJEHYgSQIO5AEYQeSIOxAEvSzI6Vuf/ePPPJIx/a1a9dWWU6l2vWzlxqy2fYhSeOSvpR0MiKWlPk9AP1TKuyFH0bEpxX8DoA+Yp8dSKJs2EPSG7Z32B6e7AO2h22P2h4tOS8AJZTdjB+KiCO2vyNpi+0DEXHGAF0RMSJpROIAHVCnUmv2iDhSPI9Jek3S0iqKAlC9nsNu+wLbc069lrRc0t6qCgNQrZ772W1fodbaXGrtDvwpIjp2PrIZj6Yoe35Jk693b9fPzkk1SClj2Ol6A5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJKgZ2BGacbkM2T0es2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCUZxxYx19dVXt23bv39/x+82eZTWbnoexdX2ettjtvdOmDbP9hbb7xfPc6ssFkD1prIZ/5ykm8+a9pCkNyPiSklvFu8BNFjXsEfEVknHz5q8QtKG4vUGSbdVXBeAivV6bvz8iDhavP5E0vx2H7Q9LGm4x/kAqEjpC2EiIjodeIuIEUkjEgfogDr12vV2zPYCSSqex6orCUA/9Br2jZJWFa9XSXq9mnIA9EvXfnbbL0q6UdLFko5JelTSnyW9LOlSSR9JuiMizj6IN9lvsRmPgSlzDslM7GfnpBrMWIT9TJwuCyRB2IEkCDuQBGEHkiDsQBLcShrT1k033dTzd6fz0fZesWYHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS46g3T1hQuzx5QJc3CVW9AcoQdSIKwA0kQdiAJwg4kQdiBJAg7kATXs6OxuvWjz5o1a0CVzAys2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCfrZURuuRx+srmt22+ttj9neO2HaGttHbO8qHrf2t0wAZU1lM/45STdPMv2piLiueGyqtiwAVesa9ojYKun4AGoB0EdlDtDdY3t3sZk/t92HbA/bHrU9WmJeAEqa0g0nbV8m6S8RcW3xfr6kTyWFpF9JWhARP5nC73DDSZzGAbr+qPSGkxFxLCK+jIivJD0jaWmZ4gD0X09ht71gwtvbJe1t91kAzdC1n932i5JulHSx7cOSHpV0o+3r1NqMPyTp7j7WiAa76qqrOrYfPHiwbRub6YPVNewRsXKSyc/2oRYAfcTpskAShB1IgrADSRB2IAnCDiTBJa7o6L777uvYvm7duo7tdK81B2t2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCfvbkpnKnok7oR58+WLMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL0s89w3frRT5482bF91qxZVZaDGrFmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk6GefBspcc8715jil65rd9iLbb9l+z/Y+2/cX0+fZ3mL7/eJ5bv/LBdArd1tr2F4gaUFE7LQ9R9IOSbdJukvS8Yh43PZDkuZGxINdfqvcbVGSYs2OcxERk/6jd12zR8TRiNhZvB6XtF/SQkkrJG0oPrZBrf8AADTUOe2z275M0vWS3pE0PyKOFk2fSJrf5jvDkoZ7LxFAFbpuxp/+oD1b0t8krY2IV23/JyK+NaH93xHRcb+dzfjesBmPc9HzZrwk2Z4l6RVJL0TEq8XkY8X+/Kn9+rEqCgXQH1M5Gm9Jz0raHxFPTmjaKGlV8XqVpNerL29miIhSD9s9P4BTpnI0fkjSNkl7JH1VTH5Yrf32lyVdKukjSXdExPEuv5VyM557s2OQ2m3GT3mfvQqEvTeEHeei1D47gOmPsANJEHYgCcIOJEHYgSS4xHWKVq9e3bbtscce6/hdjqajCVizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAS9LMXuBsMZjrW7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxIzpZ+cOrkBnrNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IImu/ey2F0l6XtJ8SSFpJCLW2V4j6WeS/lV89OGI2FSmmM2bN3dsX758edu2OXPmdPzuiRMneqoJOX322Wcd2y+88MK+zbtf53xM5aSak5J+GRE7bc+RtMP2lqLtqYj4TV8qA1CprmGPiKOSjhavx23vl7Sw34UBqNY57bPbvkzS9ZLeKSbdY3u37fW257b5zrDtUdujpSoFUMqUw257tqRXJP0iIv4r6feSvifpOrXW/L+d7HsRMRIRSyJiSQX1AujRlMJue5ZaQX8hIl6VpIg4FhFfRsRXkp6RtLR/ZQIoq2vY3To0+Kyk/RHx5ITpCyZ87HZJe6svD0BVpnI0fpmkH0vaY3tXMe1hSSttX6dWd9whSXd3+6HFixdrdLQ/u+7j4+N9+d3shoaG6i6hFhdddFHdJVRuKkfjt0uarOOvVJ86gMHiDDogCcIOJEHYgSQIO5AEYQeSIOxAEi57C+Zzmpk9uJkBSUXEpNfIsmYHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQGPWTzp5I+mvD+4mJaEzW1tqbWJVFbr6qs7bvtGgZ6Us3XZm6PNvXedE2tral1SdTWq0HVxmY8kARhB5KoO+wjNc+/k6bW1tS6JGrr1UBqq3WfHcDg1L1mBzAghB1Iopaw277Z9kHbH9h+qI4a2rF9yPYe27vqHp+uGENvzPbeCdPm2d5i+/3iedIx9mqqbY3tI8Wy22X71ppqW2T7Ldvv2d5n+/5ieq3LrkNdA1luA99nt32epH9I+pGkw5LelbQyIt4baCFt2D4kaUlE1H4Chu0fSDoh6fmIuLaY9mtJxyPi8eI/yrkR8WBDalsj6UTdw3gXoxUtmDjMuKTbJN2lGpddh7ru0ACWWx1r9qWSPoiIDyPiC0kvSVpRQx2NFxFbJR0/a/IKSRuK1xvU+mMZuDa1NUJEHI2IncXrcUmnhhmvddl1qGsg6gj7QkkfT3h/WM0a7z0kvWF7h+3huouZxPyIOFq8/kTS/DqLmUTXYbwH6axhxhuz7HoZ/rwsDtB93VBEfF/SLZJ+XmyuNlK09sGa1Hc6pWG8B2WSYcZPq3PZ9Tr8eVl1hP2IpEUT3l9STGuEiDhSPI9Jek3NG4r62KkRdIvnsZrrOa1Jw3hPNsy4GrDs6hz+vI6wvyvpStuX2z5f0p2SNtZQx9fYvqA4cCLbF0haruYNRb1R0qri9SpJr9dYyxmaMox3u2HGVfOyq33484gY+EPSrWodkf+npNV11NCmrisk/b147Ku7NkkvqrVZ9z+1jm38VNK3Jb0p6X1Jf5U0r0G1/VHSHkm71QrWgppqG1JrE323pF3F49a6l12Hugay3DhdFkiCA3RAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMT/AS1rO+4nVLUPAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 2\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[1.1464838e-16 1.1347910e-11 9.9948198e-01 2.1163882e-22 1.9174286e-25\n",
            "  4.0055066e-21 2.1560620e-17 7.4966318e-09 1.7921379e-22 1.3877622e-19\n",
            "  1.0770382e-24 3.4958726e-24 1.4584592e-16 2.4086722e-28 1.1723132e-21\n",
            "  8.4590685e-23 5.1810546e-04 7.0553256e-28 2.4778275e-14]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 11\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[8.8744353e-15 1.0355310e-08 1.0994607e-09 5.0222571e-10 8.1433114e-08\n",
            "  2.9534788e-11 7.7248788e-14 5.6015814e-09 4.3561096e-11 6.9466422e-10\n",
            "  8.6813134e-07 8.0321354e-01 1.5342691e-10 1.5561280e-05 1.9676988e-01\n",
            "  2.3688784e-10 3.4516054e-10 2.2648973e-13 9.1417540e-09]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 11\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[8.1328024e-14 3.2008850e-07 4.4198188e-08 1.4160581e-08 1.9971752e-08\n",
            "  2.6614083e-10 3.1421795e-14 4.0627057e-08 7.9723108e-09 9.9810105e-10\n",
            "  6.5634666e-08 7.4402881e-01 2.8085279e-09 3.2438189e-03 2.5272694e-01\n",
            "  3.7185381e-09 5.0664601e-11 4.9467271e-11 8.0002316e-09]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "=========PREDICTION============ \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Output: 0\n",
            "\n",
            "Prediction (Softmax) from the neural network:\n",
            "\n",
            " [[1.0000000e+00 3.1419986e-23 1.1910469e-20 1.0534290e-17 7.9904702e-25\n",
            "  2.8774913e-23 1.7550042e-19 7.8595450e-24 1.5248621e-18 3.3565351e-18\n",
            "  0.0000000e+00 1.4790224e-28 2.0338543e-33 3.3476975e-34 5.9700031e-29\n",
            "  3.9042983e-33 4.4172244e-23 3.4148216e-25 1.9682265e-35]]\n",
            "\n",
            "\n",
            "Hard-maxed form of the prediction: \n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "equation=''\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    \n",
        "    train_data[i]=np.array(train_data[i])\n",
        "    train_data[i]=train_data[i].reshape(1,28,28,1)\n",
        "    result=np.argmax(model.predict(train_data[i]), axis=-1)\n",
        "        \n",
        "    for j in range(10) :\n",
        "        if result[0] == j :\n",
        "            equation = equation + str(j)\n",
        "    \n",
        "    if result[0] == 10 :\n",
        "        equation = equation + \"+\"\n",
        "    if result[0] == 11 :\n",
        "        equation = equation + \"-\"\n",
        "    if result[0] == 12 :\n",
        "        equation = equation + \"*\"\n",
        "    if result[0] == 13 :\n",
        "        equation = equation + \"/\"\n",
        "    if result[0] == 14 :\n",
        "        equation = equation + \"=\"\n",
        "    if result[0] == 15 :\n",
        "        equation = equation + \".\"\n",
        "    if result[0] == 16 :\n",
        "        equation = equation + \"x\"\n",
        "    if result[0] == 17 :\n",
        "        equation = equation + \"y\"      \n",
        "    if result[0] == 18 :\n",
        "        equation = equation + \"z\"\n",
        "    \n",
        "print(\"Your Equation :\", equation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPC1VYaDmCwH",
        "outputId": "ba38473d-20c8-44b8-dc6c-05ec464b59d5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Equation : 6x+2--0\n"
          ]
        }
      ]
    }
  ]
}